{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a48e775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GenAI-Practice\\env_RAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='D:/GenAI-Practice/AgenticAI-Projects/SmartAssistant/.env')\n",
    "\n",
    "# Initialize model (replace with your working one, e.g., gemini-2.5-pro)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
    "                             temperature=0,\n",
    "                             api_key=os.getenv('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b09fae",
   "metadata": {},
   "source": [
    "Classifier Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3d3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Literal, List, Any\n",
    "\n",
    "class ChatbotState(BaseModel):\n",
    "    \"\"\"Represents the state of the chatbot flow.\"\"\"\n",
    "    \n",
    "    query: str = Field(..., description=\"User's current query\")\n",
    "    query_type: Optional[Literal[\"llm\", \"rag\", \"db\"]] = Field(\n",
    "        default=None, description=\"Type of query determined by classifier\")\n",
    "    context: Optional[List[str]] = Field(\n",
    "        default=None, description=\"Retrieved context or documents for RAG\")\n",
    "    answer: Optional[str] = Field(\n",
    "        default=None, description=\"Final generated answer for user\")\n",
    "    reasoning_trace: Optional[str] = Field(\n",
    "        default=None, description=\"Explanation of reasoning or tool selection\")\n",
    "    memory: Optional[Any] = Field(\n",
    "        default=None, description=\"Conversation memory (optional future use)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bacd727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured output schema for LLM classification\n",
    "class QueryClassification(BaseModel):\n",
    "    query_type: Literal[\"llm\", \"rag\", \"db\"] = Field(\n",
    "        ..., description=\"Type of query (llm, rag, or db)\")\n",
    "    reasoning: str = Field(..., description=\"Explanation of why this classification was made\")\n",
    "\n",
    "# Define parser\n",
    "parser = PydanticOutputParser(pydantic_object=QueryClassification)\n",
    "\n",
    "# Define prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"You are a query classification assistant. \"\n",
    "        \"Given a user question, determine if it should be handled by:\\n\"\n",
    "        \"- 'llm': for general world knowledge or reasoning questions.\\n\"\n",
    "        \"- 'rag': for document-based or semantic search questions.\\n\"\n",
    "        \"- 'db': for database or structured data retrieval queries.\\n\\n\"\n",
    "        \"Return a structured JSON matching this schema:\\n{format_instructions}\\n\\n\"\n",
    "        \"Question: {query}\"\n",
    "    ),\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "def classify_query_node(state: ChatbotState) -> ChatbotState:\n",
    "    \"\"\"Classify query into llm, rag, or db using Gemini and structured output.\"\"\"\n",
    "    try:\n",
    "        input_prompt = prompt.format_prompt(query=state.query)\n",
    "        response = llm.invoke(input_prompt.to_string())\n",
    "        parsed = parser.parse(response.content)\n",
    "\n",
    "        state.query_type = parsed.query_type\n",
    "        state.reasoning_trace = parsed.reasoning\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        state.query_type = \"llm\"  # fallback\n",
    "        state.reasoning_trace = f\"Classification failed: {e}\"\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36834edd",
   "metadata": {},
   "source": [
    "Database Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e04f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.agent_toolkits.sql.base import SQLDatabaseToolkit\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def db_query_node(state: ChatbotState) -> ChatbotState:\n",
    "    \"\"\"\n",
    "    Handles structured data queries via LangChain SQL Agent.\n",
    "    Executes natural language questions on database and updates the state.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load database\n",
    "        db = SQLDatabase.from_uri(\"sqlite:///D:/GenAI-Practice/AgenticAI-Projects/SmartAssistant/app/data/uploads/bank_domain.db\")\n",
    "\n",
    "        # Create toolkit for SQL agent\n",
    "        toolkit = SQLDatabaseToolkit(db=db, llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\"))\n",
    "\n",
    "        # Create the SQL agent\n",
    "        agent_executor = create_sql_agent(\n",
    "            llm=toolkit.llm,\n",
    "            toolkit=toolkit,\n",
    "            verbose=True,\n",
    "            agent_type=\"openai-tools\",  # works well with Gemini too\n",
    "        )\n",
    "\n",
    "        # Run query\n",
    "        query = state.query\n",
    "        response = agent_executor.invoke({\"input\": query})\n",
    "\n",
    "        # Update chatbot state\n",
    "        state.answer = response[\"output\"]\n",
    "        state.reasoning_trace = (\n",
    "            f\"Used SQL Agent on 'sample.db' to execute query derived from user question.\"\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        state.answer = f\"Error during DB query: {e}\"\n",
    "        state.reasoning_trace = \"DB Node failed to process query.\"\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a164c49",
   "metadata": {},
   "source": [
    "RAG Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f02c41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "# from core.embedding_utils import get_huggingface_embedding\n",
    "\n",
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def get_huggingface_embedding(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Initialize and return a Hugging Face embedding model.\n",
    "    Args:\n",
    "        model_name: Name of the Hugging Face model to use.\n",
    "    Returns:\n",
    "        HuggingFaceEmbeddings instance or None if error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"🔹 Initializing Hugging Face Embedding model: {model_name}\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error initializing Hugging Face Embedding model: {e}\")\n",
    "        return None\n",
    "\n",
    "# from chatbot.state import ChatbotState\n",
    "\n",
    "def rag_node(state: ChatbotState) -> ChatbotState:\n",
    "    \"\"\"\n",
    "    Handles document-based queries using FAISS + LLM.\n",
    "    Retrieves relevant chunks and generates an answer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Load embeddings and FAISS vector store\n",
    "        embeddings = get_huggingface_embedding()\n",
    "        vectorstore = FAISS.load_local(\"D:/GenAI-Practice/AgenticAI-Projects/SmartAssistant/app/data/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "        # Step 2: Create LLM (Gemini or fallback)\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0)\n",
    "\n",
    "        # Step 3: Create Retrieval-QA chain\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "        # Step 4: Run the query\n",
    "        result = qa_chain.invoke({\"query\": state.query})\n",
    "\n",
    "        # Step 5: Update state\n",
    "        state.answer = result[\"result\"]\n",
    "        state.context = [doc.page_content for doc in result[\"source_documents\"]]\n",
    "        state.reasoning_trace = (\n",
    "            \"Used FAISS semantic search to find relevant chunks, then used Gemini for synthesis.\"\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        state.answer = f\"RAG failed: {e}\"\n",
    "        state.reasoning_trace = \"RAG node failed to process query.\"\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ba900",
   "metadata": {},
   "source": [
    "LLM Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b84bc512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from chatbot.state import ChatbotState\n",
    "# LLM = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "def llm_node(state: ChatbotState) -> ChatbotState:\n",
    "    \"\"\"\n",
    "    Handles open-domain or reasoning queries directly using Gemini.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0.7,  api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "        response = llm.invoke(state.query)\n",
    "\n",
    "        state.answer = response.content\n",
    "        state.reasoning_trace = \"Used Gemini directly for general reasoning or world knowledge question.\"\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        state.answer = f\"LLM node failed: {e}\"\n",
    "        state.reasoning_trace = \"LLM node could not generate an answer.\"\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f399d",
   "metadata": {},
   "source": [
    "Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4936cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def router(state: ChatbotState) -> str:\n",
    "    \"\"\"\n",
    "    Router node — decides which branch to take based on query_type.\n",
    "    Returns one of: 'rag', 'db', 'llm'\n",
    "    \"\"\"\n",
    "    if state.query_type == \"rag\":\n",
    "        return \"rag\"\n",
    "    elif state.query_type == \"db\":\n",
    "        return \"db\"\n",
    "    else:\n",
    "        return \"llm\"\n",
    "\n",
    "\n",
    "def build_chatbot_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Build the chatbot LangGraph pipeline with classification + routing.\n",
    "    Compatible with latest LangGraph API.\n",
    "    \"\"\"\n",
    "    graph = StateGraph(ChatbotState)\n",
    "\n",
    "    # --- Define nodes ---\n",
    "    graph.add_node(\"classify\", classify_query_node)\n",
    "    graph.add_node(\"rag\", rag_node)\n",
    "    graph.add_node(\"db\", db_query_node)\n",
    "    graph.add_node(\"llm\", llm_node)\n",
    "\n",
    "    # --- Edges ---\n",
    "    # First step always goes to classification\n",
    "    graph.set_entry_point(\"classify\")\n",
    "\n",
    "    # Conditional routing\n",
    "    graph.add_conditional_edges(\n",
    "        \"classify\",\n",
    "        router,  # function that returns key: rag / db / llm\n",
    "        {\n",
    "            \"rag\": \"rag\",\n",
    "            \"db\": \"db\",\n",
    "            \"llm\": \"llm\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # --- End edges ---\n",
    "    graph.add_edge(\"rag\", END)\n",
    "    graph.add_edge(\"db\", END)\n",
    "    graph.add_edge(\"llm\", END)\n",
    "\n",
    "    graph.set_entry_point(\"classify\")\n",
    "\n",
    "    app = graph.compile()\n",
    "\n",
    "    return app\n",
    "\n",
    "# Initialize chatbot graph\n",
    "chatbot_graph = build_chatbot_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e8800e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example invocation helper\n",
    "def run_chatbot(query: str) -> ChatbotState:\n",
    "    \"\"\"\n",
    "    Run the query through the LangGraph pipeline.\n",
    "    Returns the final ChatbotState with answer, context, reasoning_trace.\n",
    "    \"\"\"\n",
    "    initial_state = ChatbotState(query=query)\n",
    "    result_state = chatbot_graph.invoke(initial_state)\n",
    "    return result_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "202b75c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2\n",
      "Please retry in 56.81594666s. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_list_tables` with `{'tool_input': ''}`\n",
      "responded:  Finally I should write a query to answer the question about the number of bank branches.\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3maccounts, branch_locations, credit_cards, credit_exposures, customers, employees, insurance_policies, investments, loans, transactions\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How many bank branches are there?',\n",
       " 'query_type': 'db',\n",
       " 'answer': 'Error during DB query: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2\\nPlease retry in 54.626325533s. [violations {\\n}\\n, links {\\n  description: \"Learn more about Gemini API quotas\"\\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n}\\n, retry_delay {\\n  seconds: 54\\n}\\n]',\n",
       " 'reasoning_trace': 'DB Node failed to process query.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_chatbot(query = 'How many bank branches are there?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "262de2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2\n",
      "Please retry in 22.601202968s. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 22\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is capital of US?',\n",
       " 'query_type': 'llm',\n",
       " 'answer': 'LLM node failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2\\nPlease retry in 20.432528054s. [violations {\\n}\\n, links {\\n  description: \"Learn more about Gemini API quotas\"\\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n}\\n, retry_delay {\\n  seconds: 20\\n}\\n]',\n",
       " 'reasoning_trace': 'LLM node could not generate an answer.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_chatbot(query = 'What is capital of US?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a64e173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Initializing Hugging Face Embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "response = run_chatbot(query = 'Can you tell me what is encoder and decoder from the document?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49de2e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the document provided:\n",
      "\n",
      "**Encoder:**\n",
      "The encoder's role is to map an input sequence of symbol representations (x1, ..., xn) into a sequence of continuous representations, z = (z1, ..., zn). It contains self-attention layers where each position in the encoder can attend to all positions from the previous layer.\n",
      "\n",
      "**Decoder:**\n",
      "The decoder's role is to take the continuous representation **z** from the encoder and generate an output sequence of symbols (y1, ..., ym) one element at a time. It is \"auto-regressive,\" meaning it uses the symbols it has already generated as additional input to generate the next one.\n",
      "\n",
      "The decoder has three main sub-layers:\n",
      "1.  A modified self-attention layer that prevents positions from attending to subsequent positions (masking).\n",
      "2.  An \"encoder-decoder attention\" layer where it performs multi-head attention over the output of the encoder. This allows every position in the decoder to attend to all positions in the input sequence.\n",
      "3.  A third sub-layer (implied to be a feed-forward network, similar to the encoder).\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cdaf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_RAG (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
