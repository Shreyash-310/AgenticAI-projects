{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a48e775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GenAI-Practice\\env_RAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='D:/GenAI-Practice/AgenticAI-Projects/SmartAssistant/.env')\n",
    "\n",
    "# Initialize model (replace with your working one, e.g., gemini-2.5-pro)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
    "#                              temperature=0,\n",
    "#                              api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature = 0,\n",
    "    groq_api_key = os.getenv(\"groq_api_key\"),\n",
    "    model_name = os.getenv(\"llama_model_name\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b09fae",
   "metadata": {},
   "source": [
    "Classifier Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3d3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Literal, List, Any\n",
    "\n",
    "class ChatbotState(BaseModel):\n",
    "    \"\"\"Represents the state of the chatbot flow.\"\"\"\n",
    "    \n",
    "    query: str = Field(..., description=\"User's current query\")\n",
    "    query_type: Optional[Literal[\"llm\", \"rag\", \"db\"]] = Field(\n",
    "        default=None, description=\"Type of query determined by classifier\")\n",
    "    context: Optional[List[str]] = Field(\n",
    "        default=None, description=\"Retrieved context or documents for RAG\")\n",
    "    answer: Optional[str] = Field(\n",
    "        default=None, description=\"Final generated answer for user\")\n",
    "    reasoning_trace: Optional[str] = Field(\n",
    "        default=None, description=\"Explanation of reasoning or tool selection\")\n",
    "    memory: Optional[Any] = Field(\n",
    "        default=None, description=\"Conversation memory (optional future use)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bacd727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured output schema for LLM classification\n",
    "class QueryClassification(BaseModel):\n",
    "    query_type: Literal[\"llm\", \"rag\", \"db\"] = Field(\n",
    "        ..., description=\"Type of query (llm, rag, or db)\")\n",
    "    reasoning: str = Field(..., description=\"Explanation of why this classification was made\")\n",
    "\n",
    "# Define parser\n",
    "parser = PydanticOutputParser(pydantic_object=QueryClassification)\n",
    "\n",
    "# Define prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"You are a query classification assistant. \"\n",
    "        \"Given a user question, determine if it should be handled by:\\n\"\n",
    "        \"- 'llm': for general world knowledge or reasoning questions.\\n\"\n",
    "        \"- 'rag': for document-based or semantic search questions.\\n\"\n",
    "        \"- 'db': for database or structured data retrieval queries. Mostly queries will be based on the retrieving the system data.\\neg can you tell the system value \\n\\n\"\n",
    "        \"Return a structured JSON matching this schema:\\n{format_instructions}\\n\\n\"\n",
    "        \"Question: {query}\"\n",
    "    ),\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "def classify_query_node(state: ChatbotState) -> ChatbotState:\n",
    "    \"\"\"Classify query into llm, rag, or db using Gemini and structured output.\"\"\"\n",
    "    try:\n",
    "        input_prompt = prompt.format_prompt(query=state.query)\n",
    "        response = llm.invoke(input_prompt.to_string())\n",
    "        parsed = parser.parse(response.content)\n",
    "\n",
    "        state.query_type = parsed.query_type\n",
    "        state.reasoning_trace = parsed.reasoning\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        state.query_type = \"llm\"  # fallback\n",
    "        state.reasoning_trace = f\"Classification failed: {e}\"\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36834edd",
   "metadata": {},
   "source": [
    "Database Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e04f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.agent_toolkits.sql.base import SQLDatabaseToolkit\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def db_query_node(state: ChatbotState) -> ChatbotState:\n",
    "    \"\"\"\n",
    "    Handles structured data queries via LangChain SQL Agent.\n",
    "    Executes natural language questions on database and updates the state.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load database\n",
    "        db = SQLDatabase.from_uri(\"sqlite:///D:/GenAI-Practice/AgenticAI-Projects/SmartAssistant/app/data/uploads/system_details.db\")\n",
    "\n",
    "        # Create toolkit for SQL agent\n",
    "        toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "        # Create the SQL agent\n",
    "        agent_executor = create_sql_agent(\n",
    "            llm=toolkit.llm,\n",
    "            toolkit=toolkit,\n",
    "            verbose=True,\n",
    "            agent_type=\"openai-tools\",  # works well with Gemini too\n",
    "        )\n",
    "\n",
    "        # Run query\n",
    "        query = state.query\n",
    "        response = agent_executor.invoke({\"input\": query})\n",
    "\n",
    "        # Update chatbot state\n",
    "        state.answer = response[\"output\"]\n",
    "        state.reasoning_trace = (\n",
    "            f\"Used SQL Agent on 'sample.db' to execute query derived from user question.\"\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        state.answer = f\"Error during DB query: {e}\"\n",
    "        state.reasoning_trace = \"DB Node failed to process query.\"\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a164c49",
   "metadata": {},
   "source": [
    "RAG Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "# from core.embedding_utils import get_huggingface_embedding\n",
    "\n",
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def get_huggingface_embedding(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Initialize and return a Hugging Face embedding model.\n",
    "    Args:\n",
    "        model_name: Name of the Hugging Face model to use.\n",
    "    Returns:\n",
    "        HuggingFaceEmbeddings instance or None if error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"🔹 Initializing Hugging Face Embedding model: {model_name}\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error initializing Hugging Face Embedding model: {e}\")\n",
    "        return None\n",
    "\n",
    "# from chatbot.state import ChatbotState\n",
    "\n",
    "def rag_node(state: ChatbotState) -> ChatbotState:\n",
    "    \"\"\"\n",
    "    Handles document-based queries using FAISS + LLM.\n",
    "    Retrieves relevant chunks and generates an answer.\n",
    "    \"\"\"\n",
    "    global llm\n",
    "    try:\n",
    "        # Step 1: Load embeddings and FAISS vector store\n",
    "        embeddings = get_huggingface_embedding()\n",
    "        vectorstore = FAISS.load_local(\"D:/GenAI-Practice/AgenticAI-Projects/SmartAssistant/app/data/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "        # Step 2: Create LLM (Gemini or fallback)\n",
    "        llm = llm\n",
    "\n",
    "        # Step 3: Create Retrieval-QA chain\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "        # Step 4: Run the query\n",
    "        result = qa_chain.invoke({\"query\": state.query})\n",
    "\n",
    "        # Step 5: Update state\n",
    "        state.answer = result[\"result\"]\n",
    "        state.context = [doc.page_content for doc in result[\"source_documents\"]]\n",
    "        state.reasoning_trace = (\n",
    "            \"Used FAISS semantic search to find relevant chunks, then used Gemini for synthesis.\"\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        state.answer = f\"RAG failed: {e}\"\n",
    "        state.reasoning_trace = \"RAG node failed to process query.\"\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54fd7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_node_test(query):\n",
    "    \"\"\"\n",
    "    Handles document-based queries using FAISS + LLM.\n",
    "    Retrieves relevant chunks and generates an answer.\n",
    "    \"\"\"\n",
    "    global llm\n",
    "    try:\n",
    "        # Step 1: Load embeddings and FAISS vector store\n",
    "        embeddings = get_huggingface_embedding()\n",
    "        vectorstore = FAISS.load_local(\"D:/GenAI-Practice/AgenticAI-Projects/SmartAssistant/app/data/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "        # Step 2: Create LLM (Gemini or fallback)\n",
    "        llm = llm\n",
    "\n",
    "        # Step 3: Create Retrieval-QA chain\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "        # Step 4: Run the query\n",
    "        result = qa_chain.invoke({\"query\": query})\n",
    "        state = {}\n",
    "        # Step 5: Update state\n",
    "        state['answer'] = result[\"result\"]\n",
    "        state['context'] = [doc.page_content for doc in result[\"source_documents\"]]\n",
    "        state['reasoning_trace'] = (\n",
    "            \"Used FAISS semantic search to find relevant chunks, then used Gemini for synthesis.\"\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        state.answer = f\"RAG failed: {e}\"\n",
    "        state.reasoning_trace = \"RAG node failed to process query.\"\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5850e231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Initializing Hugging Face Embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'According to the text, a Participant who desires to withdraw as a Contributing Participant will need to give notice of withdrawal to the Plan Administrator at least 30 days (or such shorter period as the Plan Administrator will permit in a uniform and nondiscriminatory manner) before the effective date of withdrawal.',\n",
       " 'context': ['to be effective  . Such amendment will take effect unless within the \\n30-day period after such notice is provided, or within such shorter \\nperiod as the notice may specify, the Adopting Employer gives the \\nPre-approved Document Provider written notice of refusal to consent \\nto the amendment  . Such written notice of refusal will have the effect \\nof withdrawing the Plan as a pre-approved plan and will cause the \\nPlan to be considered an individually designed plan  .\\n3  . In addition to the amendment rights described above, the Pre-\\napproved Document Provider will have the right to terminate its spon -\\nsorship of this Plan by providing notice (either in writing or in any \\nother form permitted under rules promulgated by the IRS and DOL) to \\nthe Adopting Employer of such termination  . Such termination of \\nsponsorship will have the effect of withdrawing the Plan as a pre-\\napproved plan and will cause the Plan to be considered an individu -',\n",
       "  'Deferrals (or Nondeductible Employee Contributions) and thus withdraw \\nas a Contributing Participant as of any such times established by the \\nPlan Administrator in a uniform and nondiscriminatory manner by revok -\\ning the authorization to the Employer to make Elective Deferrals (or \\nNondeductible Employee Contributions) on their behalf  . A Participant \\nwho desires to withdraw as a Contributing Participant will give notice of \\nwithdrawal to the Plan Administrator at least 30 days (or such shorter \\nperiod as the Plan Administrator will permit in a uniform and nondis -\\ncriminatory manner) before the effective date of withdrawal  . A \\nParticipant will cease to be a Contributing Participant upon their \\nTermination of Employment or on account of termination of the Plan  .\\nC. Return as a Contributing Participant After Ceasing Elective \\nDeferrals— A Participant who has withdrawn as a Contributing \\nParticipant (e  . g  . , pursuant to Plan Section 3  . 01(B), a suspension due to',\n",
       "  'If the claim involves disability benefits under the Plan, the Participant or \\nBeneficiary will have 180 days from receipt of the denial notice in which \\nto make written application for review by the Plan Administrator  . The \\nPlan Administrator shall issue a decision on such review within 45 days \\nafter receipt of an application for review as provided for in this Plan \\nSection 5  .09 and pursuant to Department of Labor regulation section \\n2560 .503-1 .\\nUpon a decision unfavorable to the Participant or Beneficiary, such \\nParticipant or Beneficiary will be entitled to bring such actions in law or \\nequity as may be necessary or appropriate to protect or clarify their \\nright to benefits under this Plan  . The Participant or Beneficiary will have \\none year from receipt of the denial notice to bring such action  .\\n5.10  Joint and Survivor Annuity Requirements\\nA. Application— The provisions of this Plan Section 5  .10 will apply to'],\n",
       " 'reasoning_trace': 'Used FAISS semantic search to find relevant chunks, then used Gemini for synthesis.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query = 'what is survivor annunuity percentage for the plan from document?'\n",
    "Query = 'How many days of notice is needed to withdraw from plan?'\n",
    "rag_ans = rag_node_test(query = Query)\n",
    "rag_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ba900",
   "metadata": {},
   "source": [
    "LLM Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b84bc512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from chatbot.state import ChatbotState\n",
    "# LLM = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "def llm_node(state: ChatbotState) -> ChatbotState:\n",
    "    \"\"\"\n",
    "    Handles open-domain or reasoning queries directly using Gemini.\n",
    "    \"\"\"\n",
    "    global llm\n",
    "    try:\n",
    "        llm = llm # ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0.7,  api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "        response = llm.invoke(state.query)\n",
    "\n",
    "        state.answer = response.content\n",
    "        state.reasoning_trace = \"Used Gemini directly for general reasoning or world knowledge question.\"\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        state.answer = f\"LLM node failed: {e}\"\n",
    "        state.reasoning_trace = \"LLM node could not generate an answer.\"\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f399d",
   "metadata": {},
   "source": [
    "Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4936cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def router(state: ChatbotState) -> str:\n",
    "    \"\"\"\n",
    "    Router node — decides which branch to take based on query_type.\n",
    "    Returns one of: 'rag', 'db', 'llm'\n",
    "    \"\"\"\n",
    "    if state.query_type == \"rag\":\n",
    "        return \"rag\"\n",
    "    elif state.query_type == \"db\":\n",
    "        return \"db\"\n",
    "    else:\n",
    "        return \"llm\"\n",
    "\n",
    "\n",
    "def build_chatbot_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Build the chatbot LangGraph pipeline with classification + routing.\n",
    "    Compatible with latest LangGraph API.\n",
    "    \"\"\"\n",
    "    graph = StateGraph(ChatbotState)\n",
    "\n",
    "    # --- Define nodes ---\n",
    "    graph.add_node(\"classify\", classify_query_node)\n",
    "    graph.add_node(\"rag\", rag_node)\n",
    "    graph.add_node(\"db\", db_query_node)\n",
    "    graph.add_node(\"llm\", llm_node)\n",
    "\n",
    "    # --- Edges ---\n",
    "    # First step always goes to classification\n",
    "    graph.set_entry_point(\"classify\")\n",
    "\n",
    "    # Conditional routing\n",
    "    graph.add_conditional_edges(\n",
    "        \"classify\",\n",
    "        router,  # function that returns key: rag / db / llm\n",
    "        {\n",
    "            \"rag\": \"rag\",\n",
    "            \"db\": \"db\",\n",
    "            \"llm\": \"llm\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # --- End edges ---\n",
    "    graph.add_edge(\"rag\", END)\n",
    "    graph.add_edge(\"db\", END)\n",
    "    graph.add_edge(\"llm\", END)\n",
    "\n",
    "    graph.set_entry_point(\"classify\")\n",
    "\n",
    "    app = graph.compile()\n",
    "\n",
    "    return app\n",
    "\n",
    "# Initialize chatbot graph\n",
    "chatbot_graph = build_chatbot_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e8800e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example invocation helper\n",
    "def run_chatbot(query: str) -> ChatbotState:\n",
    "    \"\"\"\n",
    "    Run the query through the LangGraph pipeline.\n",
    "    Returns the final ChatbotState with answer, context, reasoning_trace.\n",
    "    \"\"\"\n",
    "    initial_state = ChatbotState(query=query)\n",
    "    result_state = chatbot_graph.invoke(initial_state)\n",
    "    return result_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22621d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Initializing Hugging Face Embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "response = run_chatbot(query = 'what is survivor annunuity percentage for the plan from document?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "202b75c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_list_tables` with `{'tool_input': ''}`\n",
      "responded:   Then I should double check my query before running it.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3msystem_info\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_schema` with `{'table_names': 'plan, survivor_annuity'}`\n",
      "responded:   Then I should double check my query before running it.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mError: table_names {'plan', 'survivor_annuity'} not found in database\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_query_checker` with `{'query': 'SELECT survivor_annuity_percentage FROM plan LIMIT 10;'}`\n",
      "responded:   Then I should double check my query before running it.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m```sql\n",
      "SELECT survivor_annuity_percentage \n",
      "FROM plan \n",
      "LIMIT 10;\n",
      "```\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_query` with `{'query': 'SELECT survivor_annuity_percentage FROM plan LIMIT 10;'}`\n",
      "responded:   Then I should double check my query before running it.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mError: (sqlite3.OperationalError) no such table: plan\n",
      "[SQL: SELECT survivor_annuity_percentage FROM plan LIMIT 10;]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_list_tables` with `{'tool_input': ''}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3msystem_info\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_schema` with `{'table_names': 'benefit'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mError: table_names {'benefit'} not found in database\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_query_checker` with `{'query': 'SELECT survivor_annuity_percentage FROM benefit LIMIT 10;'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m```sql\n",
      "SELECT survivor_annuity_percentage \n",
      "FROM benefit \n",
      "LIMIT 10;\n",
      "```\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_query` with `{'query': 'SELECT survivor_annuity_percentage FROM benefit LIMIT 10;'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mError: (sqlite3.OperationalError) no such table: benefit\n",
      "[SQL: SELECT survivor_annuity_percentage FROM benefit LIMIT 10;]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_list_tables` with `{'tool_input': ''}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3msystem_info\u001b[0m\u001b[32;1m\u001b[1;3mI don't know.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'In the system, what is survivor annunuity percentage for the plan?',\n",
       " 'query_type': 'db',\n",
       " 'answer': \"I don't know.\",\n",
       " 'reasoning_trace': \"Used SQL Agent on 'sample.db' to execute query derived from user question.\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_chatbot(query = 'In the system, what is survivor annunuity percentage for the plan?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "262de2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is capital of US?',\n",
       " 'query_type': 'llm',\n",
       " 'answer': 'The capital of the United States is Washington, D.C. (short for District of Columbia).',\n",
       " 'reasoning_trace': 'Used Gemini directly for general reasoning or world knowledge question.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_chatbot(query = 'What is capital of US?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49de2e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The survivor annuity percentage under the Plan is 50 percent.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cdaf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_RAG (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
